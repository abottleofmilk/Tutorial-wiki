当我们要预测的是一个离散值时，做的工作就是“分类”。例如，要预测一个孩子能否成为优秀的运动员，其实就是要将他分到“好苗子”（能成为优秀的运动员）或“普通孩子”（不能成为优秀运动员）的类别。当我们要预测的是一个连续值时，做的工作就是“回归”。例如，预测一个孩子将来成为运动员的指数，计算得到的是0.99或者0.36之类的数值。

机器学习模型还可以将训练集中的数据划分为若干个组，每个组被称为一个“簇（cluster）”。这些自动形成的簇，可能对应着不同的潜在概念，例如“篮球苗子”、“长跑苗子”。这种学习方式被称为“聚类（clusting）”，它的重要特点是在学习过程中不需要用标签对训练样本进行标注。也就是说，学习过程能够根据现有训练集自动完成分类（聚类）。根据训练数据是否有标签，我们可以将学习划分为监督学习和无监督学习。

聚类是一种无监督学习，它能够将具有相似属性的对象划分到同一个集合（簇）中。聚类方法能够应用于所有对象，簇内的对象越相似，聚类算法的效果越好。



## K均值聚类的基本步骤

K均值聚类是一种将输入数据划分为k个簇的简单的聚类算法，该算法不断提取当前分类的中心点（也称为质心或重心），并最终在分类稳定时完成聚类。从本质上说，K均值聚类是一种迭代算法。K均值聚类算法的基本步骤如下：

1．随机选取k个点作为分类的中心点。

2．将每个数据点放到距离它最近的中心点所在的类中。

3．重新计算各个分类的数据点的平均值，将该平均值作为新的分类中心点。

4．重复步骤2和步骤3，直到分类稳定。

在第1步中，可以是随机选取k个点作为分类的中心点，也可以是随机生成k个并不存在于原始数据中的数据点作为分类中心点。在第3步中，提到的“距离最近”，说明要进行某种形式的距离计算。在具体实现时，可以根据需要采用不同形式的距离度量方法。当然，不同的计算方法会对算法的性能产生影响。